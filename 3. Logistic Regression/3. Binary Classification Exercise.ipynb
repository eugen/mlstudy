{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making predictions over amazon fine food reviews dataset\n",
    "\n",
    "## Predictions\n",
    "The purpose of this analysis is to make up a prediction model where we will be able to predict whether a recommendation is positive or negative. In this analysis, we will not focus on the Score, but only the positive/negative sentiment of the recommendation. \n",
    "\n",
    "To do so, we will work on Amazon's recommendation dataset, we will build a Term-doc incidence matrix using term frequency and inverse document frequency ponderation. When the data is ready, we will load it into predicitve algorithms, mainly naïve Bayesian and regression.\n",
    "\n",
    "In the end, we hope to find a \"best\" model for predicting the recommendation's sentiment.\n",
    "\n",
    "## Loading the data\n",
    "In order to load the data, we will use the SQLITE dataset where we will only fetch the Score and the recommendation summary. \n",
    "\n",
    "As we only want to get the global sentiment of the recommendations (positive or negative), we will purposefully ignore all Scores equal to 3. If the score id above 3, then the recommendation wil be set to \"postive\". Otherwise, it will be set to \"negative\". \n",
    "\n",
    "The data will be split into an training set and a test set with a test set ratio of 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from nltk.stem.porter import PorterStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check whether we have the dataset available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.core.display import display, HTML\n",
    "    \n",
    "if not os.path.isfile('database.sqlite'):\n",
    "    display(HTML(\"<h3 style='color: red'>Dataset database missing!</h3><h3> Please download it \"+\n",
    "          \"<a href='https://www.kaggle.com/snap/amazon-fine-food-reviews'>from here on Kaggle</a> \"+\n",
    "          \"and extract it to the current directory.\"))\n",
    "    raise(Exception(\"missing dataset\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>B001E4KFG0</td>\n",
       "      <td>A3SGXH7AUHU8GW</td>\n",
       "      <td>delmartian</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303862400</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>B00813GRG4</td>\n",
       "      <td>A1D87F6ZCVE5NK</td>\n",
       "      <td>dll pa</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1346976000</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>B000LQOCH0</td>\n",
       "      <td>ABXLMWJIXXAIN</td>\n",
       "      <td>Natalia Corres \"Natalia Corres\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1219017600</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id   ProductId          UserId                      ProfileName  \\\n",
       "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
       "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
       "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
       "\n",
       "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "0                     1                       1      5  1303862400   \n",
       "1                     0                       0      1  1346976000   \n",
       "2                     1                       1      4  1219017600   \n",
       "\n",
       "                 Summary                                               Text  \n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = sqlite3.connect('database.sqlite')\n",
    "\n",
    "pd.read_sql_query(\"SELECT * FROM Reviews LIMIT 3\", con)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select only what's of interest to us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "messages = pd.read_sql_query(\"\"\"\n",
    "SELECT \n",
    "  Score, \n",
    "  Summary, \n",
    "  HelpfulnessNumerator as VotesHelpful, \n",
    "  HelpfulnessDenominator as VotesTotal\n",
    "FROM Reviews \n",
    "WHERE Score != 3\"\"\", con)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we've got:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>VotesHelpful</th>\n",
       "      <th>VotesTotal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                Summary  VotesHelpful  VotesTotal\n",
       "0      5  Good Quality Dog Food             1           1\n",
       "1      1      Not as Advertised             0           0\n",
       "2      4  \"Delight\" says it all             1           1\n",
       "3      2         Cough Medicine             3           3\n",
       "4      5            Great taffy             0           0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the **Sentiment** column that turns the numeric score into either positive or negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Summary</th>\n",
       "      <th>VotesHelpful</th>\n",
       "      <th>VotesTotal</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                Summary  VotesHelpful  VotesTotal Sentiment\n",
       "0      5  Good Quality Dog Food             1           1  positive\n",
       "1      1      Not as Advertised             0           0  negative"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[\"Sentiment\"] = messages[\"Score\"].apply(lambda score: \"positive\" if score > 3 else \"negative\")\n",
    "messages.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, the **Usefulness** column turns the number of votes into either `useful` or `useless` using the formula\n",
    "`(VotesHelpful/VotesTotal) > 0.8`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TODO' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ec9fa4b33c9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmessages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Usefulness\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTODO\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmessages\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TODO' is not defined"
     ]
    }
   ],
   "source": [
    "messages[\"Usefulness\"] = TODO\n",
    "messages.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at some 5s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "messages[messages.Score == 5].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And some 1s as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TODO: select some reviews with score 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's do some exploratory data analysis with WordClouds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "# Note: you need to install wordcloud with pip. \n",
    "# On Windows, you might need a binary package obtainable from here: http://www.lfd.uci.edu/~gohlke/pythonlibs/#wordcloud\n",
    "\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "#mpl.rcParams['figure.figsize']=(8.0,6.0)    #(6.0,4.0)\n",
    "mpl.rcParams['font.size']=12                #10 \n",
    "mpl.rcParams['savefig.dpi']=100             #72 \n",
    "mpl.rcParams['figure.subplot.bottom']=.1 \n",
    "\n",
    "\n",
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stopwords,\n",
    "        max_words=200,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "    \n",
    "    fig = plt.figure(1, figsize=(8, 8))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "show_wordcloud(messages[\"Summary_Clean\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also view wordclouds for only positive or only negative entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TODO: create word cloud from negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TODO: create word cloud from positive reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from text data\n",
    "\n",
    "SciKit cannot work with words, so we'll just assign a new dimention to each word and work with word counts.\n",
    "\n",
    "See more here: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first do some cleanup\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "cleanup_re = re.compile('[^a-z]+')\n",
    "def cleanup(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    sentence = cleanup_re.sub(' ', sentence).strip()\n",
    "    return sentence\n",
    "\n",
    "messages[\"Summary_Clean\"] = messages[\"Summary\"].apply(cleanup)\n",
    "\n",
    "train, test = train_test_split(messages, test_size=0.2)\n",
    "print(\"%d items in training data, %d in test data\" % (len(train), len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# To cleanup stop words, add stop_words = STOPWORDS\n",
    "# But it seems to function better without it\n",
    "count_vect = CountVectorizer(min_df = 1, ngram_range = (1, 4))\n",
    "X_train_counts = count_vect.fit_transform(train[\"Summary_Clean\"])\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "\n",
    "X_test_counts = count_vect.transform(test[\"Summary_Clean\"])\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "y_train = train[\"Sentiment\"]\n",
    "y_test = test[\"Sentiment\"]\n",
    "\n",
    "# prepare \n",
    "prediction = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore a bit what those did"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_features = count_vect.get_feature_names()\n",
    "word_features[10000:10010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "chosen_word_idx = 99766\n",
    "chosen_word_indices = np.nonzero(X_train_counts[:,chosen_word_idx].toarray().ravel())[0]\n",
    "for i in chosen_word_indices[0:10]:\n",
    "    print(\"'%s' appears %d times in: %s\" % (\n",
    "        word_features[chosen_word_idx], \n",
    "        X_train_counts[i,chosen_word_idx],\n",
    "        train[\"Summary\"].values[i]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's now find the counts for \"gluten\" and the phrases that contain that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#TODO: find the counts for \"gluten\" and the reviews it appears in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Multinomial Naïve Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB().fit(X_train_tfidf, y_train)\n",
    "prediction['Multinomial'] = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Bernoulli Naïve Bayes model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "model = BernoulliNB().fit(X_train_tfidf, y_train)\n",
    "prediction['Bernoulli'] = model.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(C=1e5)\n",
    "logreg_result = logreg.fit(X_train_tfidf, y_train)\n",
    "prediction['Logistic'] = logreg.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Linear SVC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "linsvc = LinearSVC(C=1e5)\n",
    "linsvc_result = linsvc.fit(X_train_tfidf, y_train)\n",
    "prediction['LinearSVC'] = linsvc.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Results\n",
    "\n",
    "Before analyzing the results, let's remember what Precision and Recall are (more here https://en.wikipedia.org/wiki/Precision_and_recall)\n",
    "\n",
    "![Precision_Recall](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/525px-Precisionrecall.svg.png)\n",
    "\n",
    "## ROC Curves\n",
    "\n",
    "In order to compare our learning algorithms, let's build the ROC curve. The curve with the highest AUC value will show our \"best\" algorithm.\n",
    "\n",
    "In first data cleaning, stop-words removal has been used, but the results were much worse. Reason for this result could be that when people want to speak about what is or is not good, they use many small words like \"not\" for instance, and these words will typically be tagged as stop-words, and will be removed. This is why in the end, it was decided to keep the stop-words. For those who would like to try it by themselves, I have let the stop-words removal as a comment in the cleaning part of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def formatt(x):\n",
    "    if x == 'negative':\n",
    "        return 0\n",
    "    return 1\n",
    "vfunc = np.vectorize(formatt)\n",
    "\n",
    "cmp = 0\n",
    "colors = ['b', 'g', 'y', 'm', 'k']\n",
    "for model, predicted in prediction.items():\n",
    "    false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test.map(formatt), vfunc(predicted))\n",
    "    roc_auc = auc(false_positive_rate, true_positive_rate)\n",
    "    plt.plot(false_positive_rate, true_positive_rate, colors[cmp], label='%s: AUC %0.2f'% (model,roc_auc))\n",
    "    cmp += 1\n",
    "\n",
    "plt.title('Classifiers comparison with ROC')\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After plotting the ROC curve, it would appear that the Logistic regression method provides us with the best results, although the AUC value for this method is not outstanding... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I looks like the best are LogisticRegression and LinearSVC. Let's see the accuracy, recall and confusion matrix for these models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for model_name in [\"Logistic\", \"LinearSVC\"]:\n",
    "    print(\"Confusion matrix for %s\" % model_name)\n",
    "    print(metrics.classification_report(y_test, prediction[model_name], target_names = [\"positive\", \"negative\"]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues, labels=[\"positive\", \"negative\"]):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45)\n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, prediction['Logistic'])\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm)    \n",
    "\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_normalized, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also have a look at what the best & words are by looking at the coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = count_vect.get_feature_names()\n",
    "feature_coefs = pd.DataFrame(\n",
    "    data = list(zip(words, logreg_result.coef_[0])),\n",
    "    columns = ['feature', 'coef'])\n",
    "\n",
    "feature_coefs.sort_values(by='coef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def test_sample(model, sample):\n",
    "    sample_counts = count_vect.transform([sample])\n",
    "    sample_tfidf = tfidf_transformer.transform(sample_counts)\n",
    "    result = model.predict(sample_tfidf)[0]\n",
    "    prob = model.predict_proba(sample_tfidf)[0]\n",
    "    print(\"Sample estimated as %s: negative prob %f, positive prob %f\" % (result.upper(), prob[0], prob[1]))\n",
    "\n",
    "test_sample(logreg, \"The food was delicious, it smelled great and the taste was awesome\")\n",
    "test_sample(logreg, \"The whole experience was horrible. The smell was so bad that it literally made me sick.\")\n",
    "test_sample(logreg, \"The food was ok, I guess. The smell wasn't very good, but the taste was ok.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Now let's try to predict how helpful a review is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "show_wordcloud(messages[messages.Usefulness == \"useful\"][\"Summary_Clean\"], title = \"Useful\")\n",
    "show_wordcloud(messages[messages.Usefulness == \"useless\"][\"Summary_Clean\"], title = \"Useless\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing seems to pop out.. let's try to limit the dataset to only entries with at least 10 votes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "messages_ufn = messages[messages.VotesTotal >= 10]\n",
    "messages_ufn.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try again with the word clouds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "show_wordcloud(messages_ufn[messages_ufn.Usefulness == \"useful\"][\"Summary_Clean\"], title = \"Useful\")\n",
    "show_wordcloud(messages_ufn[messages_ufn.Usefulness == \"useless\"][\"Summary_Clean\"], title = \"Useless\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems a bit better, let's see if we can build a model though"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "train_ufn, test_ufn = train_test_split(messages_ufn, test_size=0.2)\n",
    "\n",
    "ufn_pipe = Pipeline([\n",
    "    ('vect', CountVectorizer(min_df = 1, ngram_range = (1, 4))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', LogisticRegression(C=1e5)),\n",
    "])\n",
    "\n",
    "ufn_result = ufn_pipe.fit(train_ufn[\"Summary_Clean\"], train_ufn[\"Usefulness\"])\n",
    "\n",
    "prediction['Logistic_Usefulness'] = ufn_pipe.predict(test_ufn[\"Summary_Clean\"])\n",
    "print(metrics.classification_report(test_ufn[\"Usefulness\"], prediction['Logistic_Usefulness']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also see which of the reviews are rated by our model as most helpful and least helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ufn_scores = [a[0] for a in ufn_pipe.predict_proba(train_ufn[\"Summary\"])]\n",
    "ufn_scores = zip(ufn_scores, train_ufn[\"Summary\"], train_ufn[\"VotesHelpful\"], train_ufn[\"VotesTotal\"])\n",
    "ufn_scores = sorted(ufn_scores, key=lambda t: t[0], reverse=True)\n",
    "\n",
    "# just make this into a DataFrame since jupyter renders it nicely:\n",
    "pd.DataFrame(ufn_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_ufn[\"Usefulness\"], prediction['Logistic_Usefulness'])\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_normalized, labels=[\"useful\", \"useless\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Even more complicated pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Useful to select only certain features in a dataset for forwarding through a pipeline\n",
    "# See: http://scikit-learn.org/stable/auto_examples/hetero_feature_union.html\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]\n",
    "\n",
    "train_ufn2, test_ufn2 = train_test_split(messages_ufn, test_size=0.2)\n",
    "\n",
    "ufn_pipe2 = Pipeline([\n",
    "   ('union', FeatureUnion(\n",
    "       transformer_list = [\n",
    "           ('summary', Pipeline([\n",
    "               ('textsel', ItemSelector(key='Summary_Clean')),\n",
    "               ('vect', CountVectorizer(min_df = 1, ngram_range = (1, 4))),\n",
    "               ('tfidf', TfidfTransformer())])),\n",
    "          ('score', ItemSelector(key=['Score']))\n",
    "       ],\n",
    "       transformer_weights = {\n",
    "           'summary': 0.2,\n",
    "           'score': 0.8\n",
    "       }\n",
    "   )),\n",
    "   ('model', LogisticRegression(C=1e5))\n",
    "])\n",
    "\n",
    "ufn_result2 = ufn_pipe2.fit(train_ufn2, train_ufn2[\"Usefulness\"])\n",
    "prediction['Logistic_Usefulness2'] = ufn_pipe2.predict(test_ufn2)\n",
    "print(metrics.classification_report(test_ufn2[\"Usefulness\"], prediction['Logistic_Usefulness2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_ufn2[\"Usefulness\"], prediction['Logistic_Usefulness2'])\n",
    "cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "np.set_printoptions(precision=2)\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_normalized, labels=[\"useful\", \"useless\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(ufn_result2.named_steps['model'].coef_[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, let's have a look at the best/worst words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ufn_summary_pipe = next(tr[1] for tr in ufn_result2.named_steps[\"union\"].transformer_list if tr[0]=='summary')\n",
    "ufn_words = ufn_summary_pipe.named_steps['vect'].get_feature_names()\n",
    "ufn_features = ufn_words + [\"Score\"]\n",
    "ufn_feature_coefs = pd.DataFrame(\n",
    "    data = list(zip(ufn_features, ufn_result2.named_steps['model'].coef_[0])),\n",
    "    columns = ['feature', 'coef'])\n",
    "ufn_feature_coefs.sort_values(by='coef')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"And the coefficient of the Score variable: \")\n",
    "ufn_feature_coefs[ufn_feature_coefs.feature == 'Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
